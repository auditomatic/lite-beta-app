const e="ollama-chat",t="Ollama Chat",s="/api/chat",r="POST",n={promptKey:"messages",wrapPrompt:!0,messageRole:"user"},a={text:{id:"text",label:"Text",description:"Standard text response",parameters:{},responseTransform:{contentPath:"message.content",fallbackPaths:["response","content"],errorPath:"error"}},json:{id:"json",label:"JSON",description:"Structured JSON response",parameters:{format:"json"},promptTransform:{append:"\nPlease respond with a JSON object in the form {'answer': answer}"},responseTransform:{contentPath:"message.content",fallbackPaths:["response","content"],errorPath:"error",isJSON:!0}}},o=[{pattern:".*",name:"All Ollama Models",params:{options:{type:"object",description:"Model options",basic:!0,properties:{temperature:{type:"number",description:"Sampling temperature",min:0,max:2,default:0,basic:!0},num_predict:{type:"integer",description:"Maximum tokens to generate",default:128,basic:!0,is_output_length:!0},top_k:{type:"integer",description:"Top-k sampling",basic:!1},top_p:{type:"number",description:"Nucleus sampling threshold",min:0,max:1,basic:!1},repeat_last_n:{type:"integer",description:"Look back for repetition",basic:!1},repeat_penalty:{type:"number",description:"Repetition penalty",basic:!1},seed:{type:"integer",description:"Random seed",basic:!1},stop:{type:"array",description:"Stop sequences",basic:!1},num_ctx:{type:"integer",description:"Context window size",basic:!1}}},format:{type:"string",description:"Response format",enum:["","json"],basic:!1},stream:{type:"boolean",description:"Stream responses",default:!1,basic:!1},keep_alive:{type:"string",description:"How long to keep model in memory",default:"5m",basic:!1}}}],p={id:e,name:t,endpoint:s,method:r,requestTransform:n,responseModes:a,modelRules:o};export{p as default,s as endpoint,e as id,r as method,o as modelRules,t as name,n as requestTransform,a as responseModes};
