const e="openai-chat",t="OpenAI Chat Completions",o={baseUrl:"https://api.openai.com",endpoints:{chat:"/v1/chat/completions"}},n={type:"bearer",header:"Authorization",envVar:"OPENAI_API_KEY"},s={defaultConcurrency:8,maxConcurrency:64,description:"OpenAI API - high concurrency supported"},i={"Content-Type":"application/json"},r=[{pattern:"^o[0-9]",name:"O-Series (Reasoning Models)",params:{max_completion_tokens:{type:"integer",description:"Maximum tokens for completion including reasoning",min:1,default:128,is_output_length:!0,basic:!0},reasoning_effort:{type:"string",description:"Reasoning effort level",enum:["low","medium","high"],basic:!1},seed:{type:"integer",description:"Seed for deterministic outputs",basic:!1},response_format:{type:"object",description:"Response format (text, json_object, or json_schema)",basic:!1},logprobs:{type:"boolean",description:"Include log probabilities",basic:!1,enables_features:{logprobs:{boolean_value:!0}}},top_logprobs:{type:"integer",description:"Number of top log probabilities",min:0,max:20,basic:!1},n:{type:"integer",description:"Number of completions to generate",basic:!1},user:{type:"string",description:"User identifier for tracking",basic:!1}},forbidden:["temperature","top_p","frequency_penalty","presence_penalty","stop"]},{pattern:"^gpt",name:"GPT Models",params:{temperature:{type:"number",description:"Sampling temperature",min:0,max:2,default:0,basic:!0},max_completion_tokens:{type:"integer",description:"Maximum tokens for completion",min:1,default:128,is_output_length:!0,basic:!0},top_p:{type:"number",description:"Nucleus sampling threshold",min:0,max:1,basic:!1},frequency_penalty:{type:"number",description:"Penalize frequent tokens",min:-2,max:2,basic:!1},presence_penalty:{type:"number",description:"Penalize tokens based on presence",min:-2,max:2,basic:!1},stop:{type:"array",description:"Stop sequences",basic:!1},seed:{type:"integer",description:"Seed for deterministic outputs",basic:!1},response_format:{type:"object",description:"Response format (text, json_object, or json_schema)",basic:!1},logprobs:{type:"boolean",description:"Include log probabilities",basic:!1,enables_features:{logprobs:{boolean_value:!0}}},top_logprobs:{type:"integer",description:"Number of top log probabilities",min:0,max:20,basic:!1},n:{type:"integer",description:"Number of completions to generate",basic:!1},user:{type:"string",description:"User identifier for tracking",basic:!1}}}],a={text:{id:"text",label:"Text",description:"Standard text response",parameters:{},responseTransform:{contentPath:"choices[0].message.content",fallbackPaths:["choices[0].text","message.content"],errorPath:"error.message",validationRules:{requireNonEmptyContent:!0,conditions:[{type:"emptyWithReason",path:"choices[0].finish_reason",value:"length",errorMessage:"Response incomplete: max tokens reached (consider increasing max_completion_tokens)"}]}}},json_mode:{id:"json_mode",label:"JSON Mode",description:"Structured JSON output",parameters:{response_format:{type:"json_object"}},promptTransform:{append:"\nPlease respond with a JSON object in the form {'answer': answer}"},responseTransform:{contentPath:"choices[0].message.content",fallbackPaths:["choices[0].text","message.content"],errorPath:"error.message",isJSON:!0,validationRules:{requireNonEmptyContent:!0,conditions:[{type:"emptyWithReason",path:"choices[0].finish_reason",value:"length",errorMessage:"Response incomplete: max tokens reached (consider increasing max_completion_tokens)"}]}}},function_calling:{id:"function_calling",label:"Function Calling",description:"Call functions with structured parameters",parameters:{tools:[{type:"function",function:{name:"provide_answer",description:"Provides the answer to the question",parameters:{type:"object",properties:{answer:{type:"string",description:"The answer value"}},required:["answer"]}}}],tool_choice:{type:"function",function:{name:"provide_answer"}}},responseTransform:{contentPath:"choices[0].message.tool_calls[0].function.arguments",fallbackPaths:["choices[0].message.content"],errorPath:"error.message",isJSON:!0,validationRules:{requireNonEmptyContent:!0,conditions:[{type:"emptyWithReason",path:"choices[0].finish_reason",value:"length",errorMessage:"Response incomplete: max tokens reached (consider increasing max_completion_tokens)"}]}}}},p={promptKey:"messages",wrapPrompt:!0,messageRole:"user"},c={logprobsPath:"choices[0].logprobs.content"},m={id:e,name:t,api:o,auth:n,execution:s,headers:i,modelRules:r,responseModes:a,requestTransform:p,responseExtraction:c};export{o as api,n as auth,m as default,s as execution,i as headers,e as id,r as modelRules,t as name,p as requestTransform,c as responseExtraction,a as responseModes};
