import{d as e,Z as n,W as t,Y as a,_ as s,F as r,a6 as l,a5 as i,k as o,U as p,X as c,G as d,u as m,ad as u,f,c as v,w as g,o as _,S as y,a7 as h,B as b,a9 as k}from"./vendor-BiapkIQZ.js";import{u as x,b as C,a as P,P as w}from"./designs-db-BnxPWGVY.js";import{u as S}from"./variables-db-CPg65Q8f.js";import{u as I}from"./models-db-wgnka_cR.js";import{u as A,T as E}from"./trials-db-OIuqYcu5.js";import{p as O}from"./registry-CD_hKmbW.js";import{u as T}from"./settings-db-B3gFDSc-.js";import{g as N}from"./defaultData-DyqJdH2z.js";import{M}from"./ModelSelectionTable-BiNHnxLg.js";import{P as R,F as j,_ as $,C as F,b as D}from"./index-JPKDQHeA.js";import{c as U}from"./cost-formatting-D-OPWEvg.js";import{_ as L}from"./BaseModal.vue_vue_type_style_index_0_lang-cdfHDDQp.js";const q={class:"design-selector"},z={class:"designs-list"},B=["onClick"],K={class:"design-main"},Y={class:"design-name"},G={class:"design-details"},J={key:0,class:"design-description"},V={key:1,class:"template-preview"},H={class:"design-meta"},W={class:"meta-item"},X={class:"meta-item"},Z={class:"design-main"},Q={class:"design-name"},ee={key:0,class:"empty-state"},ne={key:1,class:"loading-state"},te=$(e({__name:"DesignSelector",props:{designs:{},loading:{type:Boolean,default:!1}},emits:["select","create"],setup(e){function u(e){return Object.keys(e.variableBindings||{}).length}function f(e){if(!e.variableBindings)return"1";let n=1;for(const t of Object.values(e.variableBindings))if("direct"===t.type&&t.values)n*=t.values.length;else if("list"===t.type)return"?";return n>1e3?`${(n/1e3).toFixed(1)}k`:n.toString()}function v(e,n=150){return e.length<=n?e:e.substring(0,n)+"..."}return(e,g)=>{const _=p("a-tag"),y=p("a-button"),h=p("a-spin");return t(),n("div",q,[g[7]||(g[7]=a("div",{class:"selector-header"},[a("h3",null,"Available Designs"),a("p",{class:"selector-subtitle"},"Select a design or create a new one")],-1)),a("div",z,[(t(!0),n(r,null,l(e.designs,s=>{return t(),n("div",{key:s.id,class:"design-row",onClick:n=>e.$emit("select",s.id)},[a("div",K,[a("div",Y,i(s.name),1),a("div",G,[s.description?(t(),n("span",J,i(s.description),1)):(t(),n("span",V,i(v(s.promptTemplate,80)),1))])]),a("div",H,[o(_,{size:"small",color:(r=s.outputType,{text:"blue",number:"green",boolean:"purple"}[r]||"default")},{default:c(()=>[d(i(s.outputType),1)]),_:2},1032,["color"]),a("span",W,i(u(s))+" vars",1),a("span",X,i(f(s))+" combos",1)])],8,B);var r}),128)),a("div",{class:"design-row create-row",onClick:g[0]||(g[0]=n=>e.$emit("create"))},[a("div",Z,[a("div",Q,[o(m(R)),g[2]||(g[2]=d(" Create New Design "))]),g[3]||(g[3]=a("div",{class:"design-details"}," Start with a blank template ",-1))])])]),0!==e.designs.length||e.loading?s("",!0):(t(),n("div",ee,[o(m(j),{class:"empty-icon"}),g[5]||(g[5]=a("h4",null,"No designs available",-1)),g[6]||(g[6]=a("p",null,"Create your first design to get started",-1)),o(y,{type:"primary",onClick:g[1]||(g[1]=n=>e.$emit("create"))},{default:c(()=>g[4]||(g[4]=[d(" Create Design ")])),_:1,__:[4]})])),e.loading?(t(),n("div",ne,[o(h,{size:"large"})])):s("",!0)])}}}),[["__scopeId","data-v-4b6e1fb8"]]),ae={class:"modal-footer"},se={key:0,class:"footer-left"},re={class:"footer-actions"},le=$(e({__name:"ModalFooter",setup:e=>(e,r)=>(t(),n("div",ae,[e.$slots.left?(t(),n("div",se,[u(e.$slots,"left",{},void 0,!0)])):s("",!0),a("div",re,[u(e.$slots,"default",{},void 0,!0)])]))}),[["__scopeId","data-v-3a3879d0"]]),ie={class:"trial-top-bar"},oe={class:"top-bar-left"},pe={key:0,class:"header-label"},ce={key:1,class:"variable-summary"},de={class:"top-bar-right"},me={class:"trial-modal-content"},ue={class:"modal-main"},fe={class:"left-panel"},ve={key:0,class:"design-selector-container"},ge={key:1,class:"guide-state"},_e={key:2,class:"section dense-section"},ye={key:0,class:"param-section"},he={key:5,class:"text-secondary text-xs"},be={key:6,class:"param-description text-secondary"},ke={class:"param-section"},xe={class:"response-mode-option-compact"},Ce={class:"mode-label"},Pe={class:"mode-desc"},we={key:1,class:"param-section"},Se={key:5,class:"text-secondary text-xs"},Ie={key:6,class:"param-description text-secondary"},Ae={class:"param-section"},Ee={class:"api-preview"},Oe={class:"action-section"},Te={class:"right-panel"},Ne={class:"section dense-section"},Me={key:0,class:"cost-calculation-compact"},Re={class:"cost-row"},je={key:0,class:"cost-total"},$e={class:"bottom-panel"},Fe={class:"section dense-section"},De={key:1,class:"config-list-compact"},Ue={class:"config-compact-left"},Le={class:"config-name"},qe={class:"config-model"},ze={key:0,class:"config-param"},Be={key:1,class:"config-param"},Ke={class:"config-compact-right"},Ye={class:"config-total-cost"},Ge={key:2,class:"trial-summary-compact"},Je={class:"summary-row"},Ve={class:"summary-item"},He={class:"summary-item"},We={class:"summary-item"},Xe={class:"text-primary"},Ze={class:"summary-item"},Qe={class:"text-success"},en=$(e({__name:"TrialCreationModal",props:{trialToDuplicate:{},initialDesignId:{}},emits:["close","created"],setup(e,{emit:u}){const b=e,k=u,w=x(),E=S(),R=I(),j=A(),$=T(),D=f(""),L=f(b.initialDesignId||""),q=f(null),z=f({}),B=f("text"),K=f([]),Y=v(()=>w.designs.map(e=>({...e,variableBindings:Object.fromEntries(Object.entries(e.variableBindings).map(([e,n])=>[e,{...n,values:n.values?[...n.values]:void 0,source:n.source?{...n.source}:void 0}])),refusalWords:e.refusalWords?[...e.refusalWords]:void 0}))),G=v(()=>R.modelsByProvider),J=v(()=>Y.value.find(e=>e.id===L.value)),V=v(()=>{if(!J.value?.variableBindings)return 0;let e=1;for(const[,n]of Object.entries(J.value.variableBindings))if("list"===n.type&&n.listId){const t=E.lists.find(e=>e.id===n.listId);e*=t?.itemCount||0}else"direct"===n.type&&n.values&&(e*=n.values.length);return e}),H=v(()=>{const e=[];return Object.entries(G.value).forEach(([,n])=>{const t=n.filter(e=>e.enabled);e.push(...t)}),e}),W=v(()=>q.value?O.getBasicParameters(q.value.provider,q.value.modelId):{}),X=v(()=>q.value?O.getAdvancedParameters(q.value.provider,q.value.modelId):{}),Z=v(()=>q.value?O.getResponseModes(q.value.provider):{}),Q=v(()=>{if(!q.value)return"";try{const e=O.applyResponseMode(q.value.provider,B.value,z.value),n=C.buildAPIRequest({id:"preview",name:"Preview",provider:q.value.provider,model:q.value.modelId,params:e,created_at:new Date},"{{prompt}}",$.getApiKey(q.value.provider),$.getBaseUrl(q.value.provider));return JSON.stringify(n.body,null,2)}catch(e){return"Error generating preview"}}),ee=v(()=>D.value.trim().length>0),ne=v(()=>""!==L.value),ae=v(()=>null!==q.value),se=v(()=>V.value),re=v(()=>K.value.length*se.value),en=v(()=>{let e=0;return K.value.forEach(n=>{const t=pn(n);t&&(e+=t*se.value)}),e}),nn=v(()=>ee.value&&ne.value&&K.value.length>0&&V.value>0),tn=v(()=>{if(!q.value)return null;const e=q.value;if(!e.capabilities?.inputCostPerToken||!e.capabilities?.outputCostPerToken)return null;if(!J.value?.tokenEstimate?.avgTokens)return null;const n=J.value.tokenEstimate.avgTokens,t=P.getOutputTokenLimit(e.provider,e.modelId,z.value||{});return U(n,t,e.capabilities.inputCostPerToken,e.capabilities.outputCostPerToken)});function an(e,n){const t=z.value[e];if(t)try{const a=JSON.parse(t);if("array"===n&&!Array.isArray(a))throw new Error("Value must be an array");if("object"===n&&"object"!=typeof a)throw new Error("Value must be an object");z.value[e]=a}catch(a){console.error(`Invalid JSON for ${e}:`,a)}}function sn(e){q.value=e,z.value={},B.value="text";const n=(e,t="")=>{Object.entries(e).forEach(([e,a])=>{const s=t?`${t}.${e}`:e;"object"===a.type&&a.properties?n(a.properties,s):void 0!==a.default&&(z.value[s]=a.default)})},t=O.getParametersForModel(e.provider,e.modelId);n(t)}function rn(){}function ln(){if(!q.value)return;const e=O.applyResponseMode(q.value.provider,B.value,z.value),n=Z.value[B.value]?.label||B.value;K.value.push({name:`${q.value.displayName} (${n})`,provider:q.value.provider,modelId:q.value.modelId,parameters:{...e}}),q.value=null,z.value={},B.value="text"}function on(e){K.value.splice(e,1)}function pn(e){const n=R.getModel(e.provider,e.modelId);if(!n?.capabilities?.inputCostPerToken||!n?.capabilities?.outputCostPerToken)return null;if(!J.value?.tokenEstimate?.avgTokens)return null;const t=J.value.tokenEstimate.avgTokens,a=P.getOutputTokenLimit(n.provider,n.modelId,e.parameters||{});return U(t,a,n.capabilities.inputCostPerToken,n.capabilities.outputCostPerToken)}const cn=h();function dn(){k("close"),cn.push("/designs")}async function mn(){try{const e=await j.createTrial({name:D.value,designId:L.value,configurations:JSON.parse(JSON.stringify(K.value))});k("created",e),k("close")}catch(e){console.error("Failed to create trial:",e),alert("Failed to create trial: "+(e instanceof Error?e.message:"Unknown error"))}}return g(()=>b.trialToDuplicate,async e=>{if(e){e.designSnapshot,D.value=`${e.name} (Copy)`,await w.initialize();const n=e.designSnapshot?.originalId;L.value=n,K.value=(e.configurationSnapshots||e.configurations||[]).map(e=>({...e,id:N()}))}},{immediate:!0}),g(L,async e=>{if(!e)return;const n=Y.value.find(n=>n.id===e);if(!n)return;if(!n.tokenEstimate&&n.variableBindings&&Object.keys(n.variableBindings).length>0)try{const e=await P.calculateDesignTokens(n);await w.updateDesign(n.id,{tokenEstimate:e})}catch(l){console.warn("Failed to calculate token estimate:",l)}const t=j.trials,a=n.name.toLowerCase(),s=new RegExp(`^${a}\\s+(\\d+)$`,"i");let r=0;for(const i of t){const e=i.name.match(s);if(e){const n=parseInt(e[1]);n>r&&(r=n)}}D.value=`${a} ${r+1}`}),g(()=>b.initialDesignId,e=>{e&&(L.value=e)},{immediate:!0}),_(async()=>{await w.initialize(),await E.initialize(),await j.initialize(),0===R.enabledModels.length&&await R.ensureDefaultsEnabled()}),(e,u)=>{const f=p("a-select-option"),v=p("a-select"),g=p("a-input"),_=p("a-input-number"),h=p("a-switch"),b=p("a-textarea"),k=p("a-form-item"),x=p("a-col"),P=p("a-row"),w=p("a-form"),S=p("a-collapse-panel"),I=p("a-collapse"),A=p("a-button"),E=p("a-empty"),T=p("a-modal");return t(),y(T,{open:!0,title:null,width:"95vw",style:{top:"2.5vh",maxWidth:"none"},bodyStyle:{height:"95vh",padding:"0",overflow:"hidden"},footer:null,maskClosable:!1,onCancel:u[6]||(u[6]=n=>e.$emit("close"))},{default:c(()=>[a("div",ie,[a("div",oe,[u[7]||(u[7]=a("span",{class:"create-trial-title"},"Create Trial",-1)),u[8]||(u[8]=a("span",{class:"top-bar-separator"},"|",-1)),u[9]||(u[9]=a("span",{class:"header-label"},"Select Design",-1)),o(v,{value:L.value,"onUpdate:value":u[0]||(u[0]=e=>L.value=e),placeholder:"Select Design",size:"large",class:"design-select","dropdown-class-name":"design-select-dropdown",showSearch:"","filter-option":(e,n)=>(n?.label||n?.value||"").toLowerCase().includes(e.toLowerCase())},{default:c(()=>[(t(!0),n(r,null,l(Y.value,e=>(t(),y(f,{key:e.id,value:e.id,label:e.name},{default:c(()=>[d(i(e.name),1)]),_:2},1032,["value","label"]))),128))]),_:1},8,["value","filter-option"]),u[10]||(u[10]=a("span",{class:"header-label"},"Trial Name",-1)),o(g,{value:D.value,"onUpdate:value":u[1]||(u[1]=e=>D.value=e),placeholder:"Auto-generated...",size:"large",class:"trial-name-input"},null,8,["value"]),J.value?(t(),n("span",pe,"Design Variables")):s("",!0),J.value?(t(),n("span",ce," ("+i(V.value)+" conditions × "+i(J.value?.tokenEstimate?.avgTokens||"?")+" tokens/prompt = "+i(V.value*(J.value?.tokenEstimate?.avgTokens||0))+" input tokens) ",1)):s("",!0)]),a("div",de,[o(m(F),{onClick:u[2]||(u[2]=n=>e.$emit("close")),class:"close-icon"})])]),a("div",me,[a("div",ue,[a("div",fe,[L.value?q.value?(t(),n("div",_e,[a("h3",null,"Configure: "+i(q.value.displayName),1),Object.keys(W.value).length>0?(t(),n("div",ye,[u[12]||(u[12]=a("h4",null,"Basic Parameters",-1)),o(w,{layout:"vertical",size:"large"},{default:c(()=>[o(P,{gutter:[8,8]},{default:c(()=>[(t(!0),n(r,null,l(W.value,(e,a)=>(t(),y(x,{key:a,span:12},{default:c(()=>[o(k,{label:a,class:"dense-form-item"},{default:c(()=>["number"===e.type||"integer"===e.type?(t(),y(_,{key:0,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(t(),y(v,{key:2,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,size:"large"},{default:c(()=>[(t(!0),n(r,null,l(e.enum,e=>(t(),y(f,{key:e,value:e},{default:c(()=>[d(i(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(t(),y(h,{key:3,checked:z.value[a],"onUpdate:checked":e=>z.value[a]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(t(),y(b,{key:4,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:n=>an(a,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(t(),n("span",he,i(e.type)+" not supported ",1)):(t(),y(g,{key:1,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(t(),n("small",be,i(e.description),1)):s("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})])):s("",!0),a("div",ke,[u[13]||(u[13]=a("h4",null,"Response Mode",-1)),o(v,{value:B.value,"onUpdate:value":u[4]||(u[4]=e=>B.value=e),onChange:rn,size:"large",style:{width:"100%"}},{default:c(()=>[(t(!0),n(r,null,l(Z.value,(e,n)=>(t(),y(f,{key:n,value:n},{default:c(()=>[a("div",xe,[a("span",Ce,i(e.label),1),a("span",Pe,i(e.description),1)])]),_:2},1032,["value"]))),128))]),_:1},8,["value"])]),Object.keys(X.value).length>0?(t(),n("div",we,[o(I,{size:"small",ghost:""},{default:c(()=>[o(S,{key:"advanced",header:"Advanced Parameters"},{default:c(()=>[o(w,{layout:"vertical",size:"large"},{default:c(()=>[o(P,{gutter:[8,8]},{default:c(()=>[(t(!0),n(r,null,l(X.value,(e,a)=>(t(),y(x,{key:a,span:12},{default:c(()=>[o(k,{label:a,class:"dense-form-item"},{default:c(()=>["number"===e.type||"integer"===e.type?(t(),y(_,{key:0,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,min:e.min,max:e.max,step:"integer"===e.type?1:.1,placeholder:String(e.default),size:"large",style:{width:"100%"}},null,8,["value","onUpdate:value","min","max","step","placeholder"])):"string"!==e.type||e.enum?"string"===e.type&&e.enum?(t(),y(v,{key:2,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,size:"large"},{default:c(()=>[(t(!0),n(r,null,l(e.enum,e=>(t(),y(f,{key:e,value:e},{default:c(()=>[d(i(e),1)]),_:2},1032,["value"]))),128))]),_:2},1032,["value","onUpdate:value"])):"boolean"===e.type?(t(),y(h,{key:3,checked:z.value[a],"onUpdate:checked":e=>z.value[a]=e,size:"large"},null,8,["checked","onUpdate:checked"])):"array"===e.type||"object"===e.type?(t(),y(b,{key:4,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,placeholder:"array"===e.type?"[...]":"{...}","auto-size":{minRows:1,maxRows:3},size:"large",onBlur:n=>an(a,e.type)},null,8,["value","onUpdate:value","placeholder","onBlur"])):(t(),n("span",Se,i(e.type)+" not supported ",1)):(t(),y(g,{key:1,value:z.value[a],"onUpdate:value":e=>z.value[a]=e,placeholder:String(e.default),size:"large"},null,8,["value","onUpdate:value","placeholder"])),e.description?(t(),n("small",Ie,i(e.description),1)):s("",!0)]),_:2},1032,["label"])]),_:2},1024))),128))]),_:1})]),_:1})]),_:1})]),_:1})])):s("",!0),a("div",Ae,[o(I,{size:"small",ghost:""},{default:c(()=>[o(S,{key:"preview",header:"Raw API Preview"},{default:c(()=>[a("pre",Ee,i(Q.value),1)]),_:1})]),_:1})]),a("div",Oe,[o(A,{type:"primary",onClick:ln,disabled:!ae.value,size:"large",block:""},{default:c(()=>u[14]||(u[14]=[d(" Add Configuration ")])),_:1,__:[14]},8,["disabled"])])])):(t(),n("div",ge,u[11]||(u[11]=[a("div",{class:"guide-box"},[a("h2",null,"→ CHOOSE A MODEL →"),a("p",null,"Select any model from the right panel")],-1)]))):(t(),n("div",ve,[o(te,{designs:Y.value,loading:!1,onSelect:u[3]||(u[3]=e=>L.value=e),onCreate:dn},null,8,["designs"])]))]),a("div",Te,[a("div",Ne,[u[17]||(u[17]=a("h3",null,"Select Model",-1)),q.value&&tn.value?(t(),n("div",Me,[a("div",Re,[u[16]||(u[16]=a("span",{class:"cost-label"},"Cost per call:",-1)),a("strong",null,"$"+i(tn.value.toFixed(5)),1),se.value>0?(t(),n("span",je,[u[15]||(u[15]=d(" • Total: ")),a("strong",null,"$"+i((tn.value*se.value).toFixed(3)),1),d(" ("+i(se.value)+" calls) ",1)])):s("",!0)])])):s("",!0),o(M,{models:H.value,"selected-model":q.value,"is-loading":!1,"total-calls":V.value,design:J.value,"model-params":z.value,onModelSelected:sn,onModelConfigured:sn},null,8,["models","selected-model","total-calls","design","model-params"])])])]),a("div",$e,[a("div",Fe,[u[28]||(u[28]=a("h3",null,"Trial Configurations",-1)),0===K.value.length?(t(),y(E,{key:0,description:"No configurations added yet. Select a model and configure parameters.",style:{margin:"var(--spacing-md) 0"}},{image:c(()=>u[18]||(u[18]=[a("div",{style:{color:"var(--color-text-disabled)","font-size":"var(--font-size-xxl)"}},"⚙️",-1)])),_:1})):(t(),n("div",De,[(t(!0),n(r,null,l(K.value,(e,r)=>(t(),n("div",{key:r,class:"config-item-compact"},[a("div",Ue,[a("strong",Le,i(e.name),1),u[19]||(u[19]=a("span",{class:"config-separator"},"•",-1)),a("span",qe,i(e.provider)+":"+i(e.modelId),1),u[20]||(u[20]=a("span",{class:"config-separator"},"•",-1)),void 0!==e.parameters.temperature?(t(),n("span",ze," T="+i(e.parameters.temperature),1)):s("",!0),void 0!==e.parameters.max_tokens?(t(),n("span",Be," Max="+i(e.parameters.max_tokens),1)):s("",!0)]),a("div",Ke,[a("span",Ye,[u[21]||(u[21]=d(" Total: ")),a("strong",null,"$"+i(((pn(e)||0)*se.value).toFixed(2)),1)]),o(A,{type:"text",size:"small",onClick:e=>function(e){const n=K.value[e],t=R.getModel(n.provider,n.modelId);if(t){q.value=t,z.value=C.flattenParameters(n.parameters);const e=O.getResponseModes(n.provider);B.value="text";for(const[t,a]of Object.entries(e)){const e=a.parameters||{};if(Object.keys(e).every(e=>e in n.parameters)&&Object.keys(e).length>0){B.value=t;break}}}on(e)}(r)},{default:c(()=>u[22]||(u[22]=[d("Edit")])),_:2,__:[22]},1032,["onClick"]),o(A,{type:"text",size:"small",danger:"",onClick:e=>on(r)},{default:c(()=>u[23]||(u[23]=[d("Remove")])),_:2,__:[23]},1032,["onClick"])])]))),128))])),K.value.length>0&&V.value>0?(t(),n("div",Ge,[a("div",Je,[a("div",Ve,[u[24]||(u[24]=a("span",{class:"summary-label"},"Variables:",-1)),a("strong",null,i(se.value),1)]),a("div",He,[u[25]||(u[25]=a("span",{class:"summary-label"},"Configs:",-1)),a("strong",null,i(K.value.length),1)]),a("div",We,[u[26]||(u[26]=a("span",{class:"summary-label"},"Total Calls:",-1)),a("strong",Xe,i(re.value),1)]),a("div",Ze,[u[27]||(u[27]=a("span",{class:"summary-label"},"Cost:",-1)),a("strong",Qe,"$"+i(en.value.toFixed(2)),1)])])])):s("",!0)])]),o(le,null,{default:c(()=>[o(A,{onClick:u[5]||(u[5]=n=>e.$emit("close")),size:"large"},{default:c(()=>u[29]||(u[29]=[d(" Cancel ")])),_:1,__:[29]}),o(A,{type:"primary",onClick:mn,disabled:!nn.value,size:"large"},{default:c(()=>u[30]||(u[30]=[d(" Create Trial ")])),_:1,__:[30]},8,["disabled"])]),_:1})])]),_:1})}}}),[["__scopeId","data-v-ecfea2ca"]]),nn={class:"trial-overview"},tn={class:"overview-section"},an={class:"cost-value"},sn={class:"overview-section"},rn={class:"progress-details"},ln={class:"progress-stats"},on={class:"configurations-section"},pn={key:0,class:"model-info"},cn={key:0},dn={class:"prompt-section"},mn={key:0,class:"variables-section"},un=e({__name:"TrialDetailModal",props:{trial:{}},emits:["close","updated"],setup(e){const l=e,m=[{title:"Model",key:"model",width:200},{title:"Parameters",key:"params",width:300},{title:"Cost/Call",key:"cost",width:100,align:"right"}],u=v(()=>0===l.trial.progress.total?0:Math.round(l.trial.progress.completed/l.trial.progress.total*100));function f(e){const n=Object.entries(e).map(([e,n])=>`${e}: ${n}`).join(", ");return n.length>50?n.substring(0,50)+"...":n}function g(){const e=l.trial.variableSnapshots;return e&&0!==e.length?e.map(e=>({variable:e.variableName,listName:e.originalListName,count:e.data.itemCount})):[]}return(e,l)=>{const v=p("a-button"),_=p("a-tag"),h=p("a-descriptions-item"),b=p("a-descriptions"),k=p("a-progress"),x=p("a-typography-text"),C=p("a-table"),P=p("a-typography-paragraph"),w=p("a-list-item-meta"),S=p("a-list-item"),I=p("a-list");return t(),y(L,{"model-value":!0,title:e.trial.name,size:"full","onUpdate:modelValue":l[1]||(l[1]=n=>e.$emit("close"))},{footer:c(()=>[o(v,{onClick:l[0]||(l[0]=n=>e.$emit("close")),size:"large"},{default:c(()=>l[2]||(l[2]=[d(" Close ")])),_:1,__:[2]})]),default:c(()=>[a("div",nn,[a("div",tn,[l[3]||(l[3]=a("h3",null,"Trial Information",-1)),o(b,{column:2,size:"small",bordered:""},{default:c(()=>[o(h,{label:"Status"},{default:c(()=>{return[o(_,{color:(n=e.trial.status,{completed:"success",failed:"error",running:"processing",cancelled:"default",draft:"default",pending:"processing",paused:"warning"}[n]||"default")},{default:c(()=>[d(i(e.trial.status.toUpperCase()),1)]),_:1},8,["color"])];var n}),_:1}),o(h,{label:"Design"},{default:c(()=>[d(i(e.trial.designSnapshot.originalName),1)]),_:1}),o(h,{label:"Created"},{default:c(()=>{return[d(i((n=e.trial.created,new Date(n).toLocaleString())),1)];var n}),_:1}),o(h,{label:"Estimated Cost"},{default:c(()=>[a("span",an,"$"+i(e.trial.estimatedCost.toFixed(3)),1)]),_:1})]),_:1})]),a("div",sn,[l[4]||(l[4]=a("h3",null,"Progress",-1)),a("div",rn,[o(k,{percent:u.value,status:"failed"===e.trial.status?"exception":"active",size:"small"},null,8,["percent","status"]),a("div",ln,[o(_,null,{default:c(()=>[d(i(e.trial.progress.completed)+" / "+i(e.trial.progress.total)+" completed",1)]),_:1}),e.trial.progress.networkErrors>0?(t(),y(_,{key:0,color:"error"},{default:c(()=>[d(i(e.trial.progress.networkErrors)+" network errors ",1)]),_:1})):s("",!0)])])])]),a("div",on,[a("h3",null,"Configurations ("+i(e.trial.configurationSnapshots.length)+")",1),o(C,{columns:m,"data-source":e.trial.configurationSnapshots,pagination:!1,size:"small",scroll:{y:300},"row-key":"id"},{bodyCell:c(({column:e,record:l})=>["model"===e.key?(t(),n("div",pn,[a("strong",null,i(l.provider),1),a("small",null,i(l.modelId),1)])):s("",!0),"params"===e.key?(t(),y(x,{key:1,code:"",class:"params-preview"},{default:c(()=>[d(i(f(l.parameters)),1)]),_:2},1024)):s("",!0),"cost"===e.key?(t(),n(r,{key:2},[(t(),n("span",cn," $"+i(.001.toFixed(4)),1))],64)):s("",!0)]),_:1},8,["data-source"])]),a("div",dn,[l[5]||(l[5]=a("h3",null,"Prompt Template",-1)),o(P,{code:"",class:"prompt-template"},{default:c(()=>[d(i(e.trial.designSnapshot.promptTemplate),1)]),_:1})]),e.trial.variableSnapshots?.length?(t(),n("div",mn,[l[6]||(l[6]=a("h3",null,"Variables",-1)),o(I,{"data-source":g(),size:"small",split:!1},{renderItem:c(({item:e})=>[o(S,null,{default:c(()=>[o(w,null,{title:c(()=>[o(_,{color:"blue"},{default:c(()=>[d(i(e.variable),1)]),_:2},1024)]),description:c(()=>[a("span",null,i(e.listName)+" ("+i(e.count)+" values)",1)]),_:2},1024)]),_:2},1024)]),_:1},8,["data-source"])])):s("",!0)]),_:1},8,["title"])}}});class fn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),t=this.extractUniqueVariables(n),a=e.configurationSnapshots.map(e=>({provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:e.parameters})),s=new Set(a.map(e=>e.provider)),r={};for(const l of s){const e=O.getProvider(l);e&&(r[l]=this.buildProviderConfig(l,e))}return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:t},models:a,providerConfigs:r}}static extractUniqueVariables(e){const n={};for(const a of e)for(const[e,t]of Object.entries(a.variables))n[e]||(n[e]=new Set),n[e].add(t);const t={};for(const[a,s]of Object.entries(n))t[a]=Array.from(s).sort();return t}static buildProviderConfig(e,n){const t=n.requestTransform||{},a=n.auth||{type:"none"};let s="direct";"messages"===t.promptKey&&t.wrapPrompt?s="messages":"input"===t.promptKey&&(s="input");let r,l,i="root";"ollama-chat"===e?(i="options",r={max_tokens:"num_predict",max_completion_tokens:"num_predict"}):"ollama-generate"===e&&(i="mixed",l={root:["model","prompt","stream","format","raw"],options:["temperature","num_predict","top_k","top_p"]},r={max_tokens:"num_predict",max_completion_tokens:"num_predict"});const o=Object.values(n.responseModes||{})[0],p=this.parseResponsePath(o?.responseTransform?.contentPath),c=o?.responseTransform?.fallbackPaths?.map(e=>this.parseResponsePath(e)),d=n.api.baseUrl+(n.api.endpoints.chat||n.api.endpoints.generate||"");return{name:n.name,endpoint:d,auth:{type:a.type,header:a.header,prefix:"bearer"===a.type?"Bearer":void 0},headers:n.headers,request:{modelPrefixStrip:!0,promptFormat:s,messageRole:t.messageRole,paramLocation:i,paramRenames:r,mixedParams:l},response:{successPath:p,fallbackPaths:c,errorPath:["error","message"]}}}static parseResponsePath(e){return e?e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n}):["content"]}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=JSON.stringify(e.providerConfigs,null,4);return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Simple Mode\n=====================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script reproduces your experiment by generating API calls from variables.\nPerfect for understanding, modifying, and extending your experiments.\n\nOriginal trial: ${n}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${Object.keys(e.providerConfigs).map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${e}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${a}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Provider configurations (how to talk to each API)\nPROVIDER_CONFIGS = ${r}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef make_api_call(provider_id: str, model: str, prompt: str, params: dict) -> dict:\n    """\n    Universal API caller that handles all provider quirks.\n    \n    Returns dict with 'success', 'content', 'error', and timing info.\n    """\n    config = PROVIDER_CONFIGS[provider_id]\n    \n    # Build headers\n    headers = {"Content-Type": "application/json"}\n    \n    # Add authentication\n    auth = config["auth"]\n    if auth["type"] == "bearer":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = f"{auth['prefix']} {api_key}"\n    elif auth["type"] == "header":\n        api_key = API_KEYS.get(provider_id, "")\n        if not api_key:\n            return {"success": False, "error": f"No API key for {provider_id}"}\n        headers[auth["header"]] = api_key\n    \n    # Add provider-specific headers\n    if config.get("headers"):\n        headers.update(config["headers"])\n    \n    # Build request body\n    request = config["request"]\n    \n    # Strip provider prefix from model\n    if request.get("modelPrefixStrip"):\n        model = model.split(":", 1)[-1]\n    \n    body = {"model": model}\n    \n    # Format prompt\n    if request["promptFormat"] == "messages":\n        body["messages"] = [{"role": request.get("messageRole", "user"), "content": prompt}]\n    elif request["promptFormat"] == "direct":\n        body["prompt"] = prompt\n    elif request["promptFormat"] == "input":\n        body["input"] = prompt\n    \n    # Handle parameters\n    processed_params = params.copy()\n    \n    # Apply renames\n    if request.get("paramRenames"):\n        for old_key, new_key in request["paramRenames"].items():\n            if old_key in processed_params:\n                processed_params[new_key] = processed_params.pop(old_key)\n    \n    # Place parameters\n    if request["paramLocation"] == "root":\n        body.update(processed_params)\n    elif request["paramLocation"] == "options":\n        body["options"] = processed_params\n    elif request.get("mixedParams"):\n        mixed = request["mixedParams"]\n        for key, value in processed_params.items():\n            if key in mixed.get("root", []):\n                body[key] = value\n            else:\n                if "options" not in body:\n                    body["options"] = {}\n                body["options"][key] = value\n    \n    # Make request\n    start_time = time.time()\n    try:\n        response = requests.post(\n            config["endpoint"],\n            headers=headers,\n            json=body,\n            timeout=30\n        )\n        latency_ms = (time.time() - start_time) * 1000\n        \n        if response.ok:\n            data = response.json()\n            content = extract_from_path(data, config["response"]["successPath"])\n            \n            # Try fallback paths\n            if content is None and config["response"].get("fallbackPaths"):\n                for path in config["response"]["fallbackPaths"]:\n                    content = extract_from_path(data, path)\n                    if content is not None:\n                        break\n            \n            return {\n                "success": True,\n                "content": content or "",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n        else:\n            return {\n                "success": False,\n                "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                "latency_ms": latency_ms,\n                "status_code": response.status_code\n            }\n            \n    except Exception as e:\n        return {\n            "success": False,\n            "error": str(e),\n            "latency_ms": (time.time() - start_time) * 1000\n        }\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(\n                model_config["provider"],\n                model_config["modelId"],\n                prompt,\n                model_config["parameters"]\n            )\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": result.get("content", ""),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                "status_code": result.get("status_code", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {result['content'][:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_results_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider not in API_KEYS or not API_KEYS[provider]:\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(set(missing_keys)))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns:\n            print(f"Avg latency: {df['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class vn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=[],t=new w,a=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e);let s=0;for(const l of e.configurationSnapshots){const i=O.getProvider(l.provider);if(i)for(const o of a){s++;let a=e.designSnapshot.promptTemplate;for(const[e,n]of Object.entries(o.variables))a=a.replace(new RegExp(`{{${e}}}`,"g"),n);try{const e={id:"export-config",name:l.name,provider:l.provider,model:l.modelId,params:l.parameters,created_at:new Date},r=t.buildAPIRequest(e,a),p={};for(const[n,t]of Object.entries(r.headers))"Authorization"===n&&t.startsWith("Bearer ")?p[n]=`Bearer $${l.provider.split("-")[0].toUpperCase()}_API_KEY`:n===i.auth.header&&"header"===i.auth.type?p[n]=`$${l.provider.split("-")[0].toUpperCase()}_API_KEY`:p[n]=t;const c=this.parseResponsePath(this.getDefaultResponsePath(l.provider));n.push({id:`call_${String(s).padStart(3,"0")}`,provider:l.provider,endpoint:r.url,headers:p,body:r.body,responsePath:c,metadata:{variables:o.variables,modelName:l.modelId,configName:l.name}})}catch(r){console.warn(`Failed to build API call for ${l.name}:`,r)}}}return{apiCalls:n}}static parseResponsePath(e){return e.split(/[\.\[\]]/).filter(Boolean).map(e=>{const n=parseInt(e);return isNaN(n)?e:n})}static getDefaultResponsePath(e){switch(e){case"openai-chat":case"openrouter":return"choices[0].message.content";case"openai-responses":return"output[0].content[0].text";case"anthropic":return"content[0].text";case"ollama-chat":return"message.content";case"ollama-generate":return"response";default:return"content"}}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.apiCalls,null,4),s=[...new Set(e.apiCalls.map(e=>e.provider))];return`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Literal Mode\n======================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script contains the EXACT API calls from your experiment.\nPerfect for bit-for-bit reproduction, debugging, and comparing results.\n\nOriginal trial: ${n}\nTotal API calls: ${e.apiCalls.length}\n"""\n\nimport os\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\nAPI_KEYS = {\n${s.map(e=>{const n=e.split("-")[0].toUpperCase();return`    "${n}": os.environ.get("${n}_API_KEY", ""),`}).join("\n")}\n}\n\n# Pre-computed API calls from your experiment\nAPI_CALLS = ${a}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\ndef execute_literal_calls():\n    """Execute pre-serialized API calls exactly as specified"""\n    results = []\n    total = len(API_CALLS)\n    \n    print(f"Executing {total} pre-computed API calls...")\n    print("=" * 60)\n    \n    for i, call in enumerate(API_CALLS):\n        print(f"[{i+1}/{total}] {call['metadata']['configName']} - ", end="")\n        \n        # Replace API key placeholders in headers\n        headers = {}\n        for key, value in call["headers"].items():\n            if "\\$" in str(value):\n                # Extract provider name from placeholder\n                for provider_key, api_key in API_KEYS.items():\n                    placeholder = f"\\\${provider_key}_API_KEY"\n                    if placeholder in value:\n                        headers[key] = value.replace(placeholder, api_key)\n                        break\n                else:\n                    headers[key] = value\n            else:\n                headers[key] = value\n        \n        # Check if we have required API key\n        provider_base = call["provider"].split("-")[0].upper()\n        if provider_base in ["OPENAI", "ANTHROPIC", "OPENROUTER"] and not API_KEYS.get(provider_base):\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": f"No API key for {provider_base}",\n                "latency_ms": 0,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ No API key")\n            continue\n        \n        # Make the exact API call\n        start_time = time.time()\n        try:\n            response = requests.post(\n                call["endpoint"],\n                headers=headers,\n                json=call["body"],\n                timeout=30\n            )\n            latency_ms = (time.time() - start_time) * 1000\n            \n            if response.ok:\n                data = response.json()\n                content = extract_from_path(data, call["responsePath"])\n                \n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": content or "",\n                    "success": True,\n                    "error": "",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    "full_response": json.dumps(data)[:500],  # First 500 chars\n                    **call["metadata"]["variables"]\n                })\n                print(f"✓ {(content or '')[:30]}")\n            else:\n                results.append({\n                    "call_id": call["id"],\n                    "timestamp": datetime.now(),\n                    "provider": call["provider"],\n                    "model": call["metadata"]["modelName"],\n                    "config_name": call["metadata"]["configName"],\n                    "prompt": extract_prompt_from_body(call["body"]),\n                    "response": "",\n                    "success": False,\n                    "error": f"HTTP {response.status_code}: {response.text[:200]}",\n                    "latency_ms": latency_ms,\n                    "status_code": response.status_code,\n                    **call["metadata"]["variables"]\n                })\n                print(f"✗ HTTP {response.status_code}")\n                \n        except Exception as e:\n            latency_ms = (time.time() - start_time) * 1000\n            results.append({\n                "call_id": call["id"],\n                "timestamp": datetime.now(),\n                "provider": call["provider"],\n                "model": call["metadata"]["modelName"],\n                "config_name": call["metadata"]["configName"],\n                "prompt": extract_prompt_from_body(call["body"]),\n                "response": "",\n                "success": False,\n                "error": str(e)[:200],\n                "latency_ms": latency_ms,\n                "status_code": 0,\n                **call["metadata"]["variables"]\n            })\n            print(f"✗ {str(e)[:30]}")\n        \n        # Rate limiting\n        time.sleep(0.1)\n    \n    return results\n\ndef extract_prompt_from_body(body: dict) -> str:\n    """Extract the prompt from various request body formats"""\n    # Messages format (OpenAI, Anthropic, etc)\n    if "messages" in body and isinstance(body["messages"], list):\n        for msg in body["messages"]:\n            if msg.get("role") == "user":\n                return msg.get("content", "")\n    \n    # Direct prompt format (Ollama generate)\n    if "prompt" in body:\n        return body["prompt"]\n    \n    # Input format (OpenAI responses)\n    if "input" in body:\n        return body["input"]\n    \n    return ""\n\ndef extract_from_path(data: Any, path: List[Any]) -> Optional[str]:\n    """Extract value from nested data using a path like ['choices', 0, 'message', 'content']"""\n    try:\n        current = data\n        for key in path:\n            if isinstance(current, dict):\n                current = current[key]\n            elif isinstance(current, list):\n                current = current[int(key)]\n            else:\n                return None\n        return str(current) if current is not None else None\n    except (KeyError, IndexError, TypeError):\n        return None\n\ndef save_results(results: List[Dict[str, Any]], format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    # Drop full_response column for cleaner output (except JSON)\n    if format != "json" and "full_response" in df.columns:\n        df = df.drop(columns=["full_response"])\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_literal_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for API keys\n    required_providers = set(call["provider"].split("-")[0].upper() for call in API_CALLS)\n    missing_keys = []\n    for provider in required_providers:\n        if provider not in ["OLLAMA"] and not API_KEYS.get(provider):\n            missing_keys.append(provider)\n    \n    if missing_keys:\n        print("WARNING: Missing API keys for:", ", ".join(missing_keys))\n        print("Set them in the API_KEYS dict or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Execute all calls\n    results = execute_literal_calls()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if df['success'].any():\n            print(f"Avg latency (successful): {df[df['success']]['latency_ms'].mean():.1f}ms")\n        \n        # Group by model\n        print(f"\\nBy Model:")\n        model_summary = df.groupby('config_name')['success'].agg(['count', 'sum', 'mean'])\n        model_summary.columns = ['total', 'successful', 'success_rate']\n        print(model_summary)\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class gn{static generate(e){const n=this.extractData(e);return this.generateScript(n,e.name)}static extractData(e){const n=new E({getApiKey:()=>{},getBaseUrl:()=>{}}).generateVariableCombinations(e),t=this.extractUniqueVariables(n),a=e.configurationSnapshots.map(e=>{let n,t="text";return e.parameters.response_format?(t="json_mode",n={response_format:e.parameters.response_format}):e.parameters.tools&&(t="function_calling",n={tools:e.parameters.tools,tool_choice:e.parameters.tool_choice}),{provider:e.provider,modelId:e.modelId,displayName:e.name,parameters:this.filterCoreParams(e.parameters),responseMode:t,responseModeParams:n}}),s=new Set(a.map(e=>e.provider)),r={"openai-chat":"openai","openai-responses":"openai",anthropic:"anthropic",openrouter:"openai","ollama-chat":"ollama","ollama-generate":"ollama"},l=[...new Set(Array.from(s).map(e=>r[e]).filter(Boolean))],i={"openai-chat":"OPENAI","openai-responses":"OPENAI",anthropic:"ANTHROPIC",openrouter:"OPENROUTER","ollama-chat":"","ollama-generate":""},o=[...new Set(Array.from(s).map(e=>i[e]).filter(Boolean))];return{experiment:{promptTemplate:e.designSnapshot.promptTemplate,variables:t},models:a,providerLibraries:{required:l,apiKeys:o}}}static extractUniqueVariables(e){const n={};for(const a of e)for(const[e,t]of Object.entries(a.variables))n[e]||(n[e]=new Set),n[e].add(t);const t={};for(const[a,s]of Object.entries(n))t[a]=Array.from(s).sort();return t}static filterCoreParams(e){const n={...e};return delete n.response_format,delete n.tools,delete n.tool_choice,n}static generateScript(e,n){const t=(new Date).toISOString(),a=JSON.stringify(e.experiment.variables,null,4),s=JSON.stringify(e.models,null,4),r=["import os","import json","import time","import pandas as pd","from datetime import datetime"];return e.providerLibraries.required.includes("openai")&&r.push("from openai import OpenAI"),e.providerLibraries.required.includes("anthropic")&&r.push("from anthropic import Anthropic"),e.providerLibraries.required.includes("ollama")&&r.push("import ollama"),`#!/usr/bin/env python3\n"""\nAI Model Testing Script - Native Mode\n=====================================\nGenerated by Auditomatic Lite on ${t}\n\nThis script uses native Python libraries for each provider.\nCleanest code, best for production use.\n\nOriginal trial: ${n}\nRequired packages: ${e.providerLibraries.required.join(", ")}\n"""\n\n${r.join("\n")}\n\n# === CONFIGURATION ===\n\n# API Keys - Add your keys here or set as environment variables\n${e.providerLibraries.apiKeys.map(e=>`os.environ.setdefault("${e}_API_KEY", "")  # Set your ${e} API key`).join("\n")}\n\n# Your experiment design\nEXPERIMENT = {\n    "prompt_template": "${e.experiment.promptTemplate.replace(/"/g,'\\"')}",\n    "variables": ${a}\n}\n\n# Models to test\nMODELS = ${s}\n\n# Output settings\nOUTPUT_FORMAT = "csv"  # Options: csv, excel, json, parquet, html, markdown, stata, pickle\n\n# === IMPLEMENTATION ===\n\n# Initialize clients\nclients = {}\n\ndef get_client(provider):\n    """Get or create client for provider"""\n    if provider not in clients:\n        if provider in ["openai-chat", "openai-responses"]:\n            clients[provider] = OpenAI()\n        elif provider == "anthropic":\n            clients[provider] = Anthropic()\n        elif provider == "openrouter":\n            clients[provider] = OpenAI(\n                api_key=os.environ.get("OPENROUTER_API_KEY"),\n                base_url="https://openrouter.ai/api/v1"\n            )\n        # Ollama doesn't need a client\n    return clients.get(provider)\n\ndef make_api_call(model_config: dict, prompt: str) -> dict:\n    """Make API call using native provider library"""\n    provider = model_config["provider"]\n    model = model_config["modelId"]\n    params = model_config["parameters"].copy()\n    \n    try:\n        start_time = time.time()\n        \n        if provider == "openai-chat" or provider == "openrouter":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["response_format"] = {"type": "json_object"}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.chat.completions.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content based on response mode\n            if model_config["responseMode"] == "function_calling" and response.choices[0].message.tool_calls:\n                content = response.choices[0].message.tool_calls[0].function.arguments\n                if isinstance(content, str):\n                    content = json.loads(content)\n            else:\n                content = response.choices[0].message.content\n            \n        elif provider == "openai-responses":\n            client = get_client(provider)\n            \n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["text"] = {"format": {"type": "json_object"}}\n            elif model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.responses.create(\n                model=model,\n                input=prompt,\n                **params\n            )\n            \n            # Extract content\n            output = response.output\n            if isinstance(output, list) and len(output) > 0:\n                if hasattr(output[0], 'content') and isinstance(output[0].content, list):\n                    content = output[0].content[0].text if hasattr(output[0].content[0], 'text') else str(output[0].content[0])\n                else:\n                    content = str(output[0])\n            else:\n                content = str(output)\n            \n        elif provider == "anthropic":\n            client = get_client(provider)\n            \n            # Build messages\n            messages = [{"role": "user", "content": prompt}]\n            \n            # Handle response modes\n            if model_config["responseMode"] == "function_calling":\n                params.update(model_config.get("responseModeParams", {}))\n            \n            # Make call\n            response = client.messages.create(\n                model=model,\n                messages=messages,\n                **params\n            )\n            \n            # Extract content\n            if model_config["responseMode"] == "function_calling" and hasattr(response.content[0], 'input'):\n                content = response.content[0].input\n            else:\n                content = response.content[0].text\n            \n        elif provider == "ollama-chat":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.chat(\n                model=model,\n                messages=[{"role": "user", "content": prompt}],\n                **params\n            )\n            \n            # Extract content\n            content = response["message"]["content"]\n            \n        elif provider == "ollama-generate":\n            # Handle response modes\n            if model_config["responseMode"] == "json_mode":\n                params["format"] = "json"\n            \n            # Make call\n            response = ollama.generate(\n                model=model,\n                prompt=prompt,\n                **params\n            )\n            \n            # Extract content\n            content = response["response"]\n        \n        else:\n            raise ValueError(f"Unknown provider: {provider}")\n        \n        latency_ms = (time.time() - start_time) * 1000\n        \n        return {\n            "success": True,\n            "content": content,\n            "latency_ms": latency_ms\n        }\n        \n    except Exception as e:\n        latency_ms = (time.time() - start_time) * 1000\n        return {\n            "success": False,\n            "content": "",\n            "error": str(e),\n            "latency_ms": latency_ms\n        }\n\ndef generate_prompts():\n    """Generate all prompts from template and variables"""\n    template = EXPERIMENT["prompt_template"]\n    variables = EXPERIMENT["variables"]\n    \n    # Get variable names from template\n    import re\n    var_names = re.findall(r'{{(\\w+)}}', template)\n    \n    # Generate all combinations\n    from itertools import product\n    \n    var_lists = [variables[var] for var in var_names]\n    for values in product(*var_lists):\n        var_dict = dict(zip(var_names, values))\n        \n        # Replace variables in template\n        prompt = template\n        for var, val in var_dict.items():\n            prompt = prompt.replace(f"{{{{{var}}}}}", str(val))\n        \n        yield prompt, var_dict\n\ndef run_experiment():\n    """Run the full experiment"""\n    results = []\n    total_calls = len(MODELS) * len(list(generate_prompts()))\n    current = 0\n    \n    print(f"Running experiment with {len(MODELS)} models and {total_calls} total API calls")\n    print("=" * 60)\n    \n    for model_config in MODELS:\n        print(f"\\nTesting {model_config['displayName']}...")\n        \n        for prompt, variables in generate_prompts():\n            current += 1\n            print(f"[{current}/{total_calls}] {prompt[:50]}...", end=" ")\n            \n            # Make API call\n            result = make_api_call(model_config, prompt)\n            \n            # Collect results\n            results.append({\n                "timestamp": datetime.now(),\n                "provider": model_config["provider"],\n                "model": model_config["modelId"],\n                "model_name": model_config["displayName"],\n                "prompt": prompt,\n                "response": str(result.get("content", "")),\n                "success": result.get("success", False),\n                "error": result.get("error", ""),\n                "latency_ms": result.get("latency_ms", 0),\n                **variables  # Add variables as columns\n            })\n            \n            # Show result\n            if result["success"]:\n                print(f"✓ {str(result['content'])[:30]}")\n            else:\n                print(f"✗ {result['error'][:30]}")\n            \n            # Rate limiting\n            time.sleep(0.1)\n    \n    return results\n\ndef save_results(results: list, format: str = OUTPUT_FORMAT):\n    """Save results using pandas in the specified format"""\n    df = pd.DataFrame(results)\n    \n    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\n    base_filename = f"experiment_native_{timestamp}"\n    \n    if format == "csv":\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    elif format == "excel":\n        filename = f"{base_filename}.xlsx"\n        df.to_excel(filename, index=False)\n    elif format == "json":\n        filename = f"{base_filename}.json"\n        df.to_json(filename, orient="records", indent=2)\n    elif format == "parquet":\n        filename = f"{base_filename}.parquet"\n        df.to_parquet(filename)\n    elif format == "html":\n        filename = f"{base_filename}.html"\n        df.to_html(filename, index=False)\n    elif format == "markdown":\n        filename = f"{base_filename}.md"\n        with open(filename, "w") as f:\n            f.write(df.to_markdown(index=False))\n    elif format == "stata":\n        filename = f"{base_filename}.dta"\n        df.to_stata(filename)\n    elif format == "pickle":\n        filename = f"{base_filename}.pkl"\n        df.to_pickle(filename)\n    else:\n        filename = f"{base_filename}.csv"\n        df.to_csv(filename, index=False)\n    \n    print(f"\\nResults saved to {filename}")\n    return filename\n\ndef main():\n    """Main entry point"""\n    # Check for required packages\n    required = ${JSON.stringify(e.providerLibraries.required)}\n    missing = []\n    for lib in required:\n        try:\n            __import__(lib)\n        except ImportError:\n            missing.append(lib)\n    \n    if missing:\n        print(f"ERROR: Missing required packages: {', '.join(missing)}")\n        print(f"Install with: pip install {' '.join(missing)}")\n        return\n    \n    # Check for API keys\n    missing_keys = []\n    for model in MODELS:\n        provider = model["provider"]\n        if provider in ["openai-chat", "openai-responses"] and not os.environ.get("OPENAI_API_KEY"):\n            missing_keys.append("OPENAI_API_KEY")\n        elif provider == "anthropic" and not os.environ.get("ANTHROPIC_API_KEY"):\n            missing_keys.append("ANTHROPIC_API_KEY")\n        elif provider == "openrouter" and not os.environ.get("OPENROUTER_API_KEY"):\n            missing_keys.append("OPENROUTER_API_KEY")\n    \n    if missing_keys:\n        print(f"WARNING: Missing API keys: {', '.join(set(missing_keys))}")\n        print("Set them in the script or as environment variables.")\n        response = input("\\nContinue anyway? (y/N): ")\n        if response.lower() != 'y':\n            return\n    \n    # Run experiment\n    results = run_experiment()\n    \n    # Save results\n    if results:\n        save_results(results)\n        \n        # Basic summary\n        df = pd.DataFrame(results)\n        print(f"\\nSummary:")\n        print(f"Total calls: {len(df)}")\n        print(f"Successful: {df['success'].sum()}")\n        print(f"Failed: {(~df['success']).sum()}")\n        if 'latency_ms' in df.columns and df['success'].any():\n            print(f"Avg latency: {df[df['success']]['latency_ms'].mean():.1f}ms")\n    else:\n        print("\\nNo results to save")\n\nif __name__ == "__main__":\n    main()\n`}}class _n{static async generatePythonScript(e,n){try{const t=n||this.getDefaultOptions(),a=this.validateTrialForExport(e);if(!a.valid)throw new Error(`Trial validation failed: ${a.errors.join(", ")}`);switch(t.mode){case"simple":return fn.generate(e);case"literal":return vn.generate(e);case"native":return gn.generate(e);default:throw new Error(`Unknown export mode: ${t.mode}`)}}catch(t){throw new Error(`Failed to generate Python export: ${t instanceof Error?t.message:String(t)}`)}}static async downloadPythonScript(e,n){const t=await this.generatePythonScript(e,n),a=n||this.getDefaultOptions(),s=new Blob([t],{type:"text/x-python"}),r=URL.createObjectURL(s),l=document.createElement("a");l.href=r,l.download=this.generateFilename(e,a.mode),document.body.appendChild(l),l.click(),document.body.removeChild(l),URL.revokeObjectURL(r)}static validateTrialForExport(e){const n=[];return e.designSnapshot?e.designSnapshot.promptTemplate||n.push("Design missing prompt template"):n.push("Trial missing design snapshot"),e.configurationSnapshots&&0!==e.configurationSnapshots.length?e.configurationSnapshots.forEach((e,t)=>{e.provider||n.push(`Configuration ${t+1} missing provider`),e.modelId||n.push(`Configuration ${t+1} missing model`),e.parameters||n.push(`Configuration ${t+1} missing parameters`)}):n.push("Trial missing model configurations"),e.variableSnapshots||n.push("Trial missing variable snapshots"),{valid:0===n.length,errors:n}}static getExportSummary(e){const n=new Set(e.configurationSnapshots.map(e=>e.provider)),t=e.totalCombinations||0;return{apiCallCount:e.configurationSnapshots.length*t,providersUsed:Array.from(n),variableCombinations:t,configurations:e.configurationSnapshots.length}}static getDefaultOptions(){return{mode:"simple"}}static generateFilename(e,n){const t=e.name||`trial_${e.id}`,a=(new Date).toISOString().split("T")[0];return`${t.toLowerCase().replace(/[^a-z0-9]/g,"_")}_${n}_${a}.py`}}const yn=Object.freeze(Object.defineProperty({__proto__:null,PythonExportService:_n},Symbol.toStringTag,{value:"Module"})),hn={class:"trial-info"},bn={class:"trial-stats"},kn={class:"export-section"},xn={class:"mode-content"},Cn={class:"mode-content"},Pn={class:"mode-content"},wn={class:"export-section"},Sn={class:"preview-content"},In={class:"preview-info"},An=e({__name:"PythonExportModal",props:{trial:{}},emits:["close","exported"],setup(e,{emit:n}){const s=e,r=n,l=f("simple"),m=f(!1),u=v(()=>s.trial.progress.total),g=v(()=>s.trial.configurationSnapshots?.length||0),_=v(()=>s.trial.totalCombinations||0),h=v(()=>{const e=.05*_.value+.3*g.value;return Math.round(15+e)}),b=v(()=>{const e=.5*u.value;return Math.round(10+e)}),k=v(()=>{const e=.05*_.value+.2*g.value;return Math.round(12+e)}),x=v(()=>{const e=s.trial.name.toLowerCase().replace(/\s+/g,"_"),n=(new Date).toISOString().split("T")[0];return`${e}_${l.value}_${n}.py`}),C=v(()=>{if("simple"===l.value){return 300+(_.value+10*g.value)}if("native"===l.value){return 250+(_.value+8*g.value)}return 200+15*u.value});async function P(){m.value=!0;try{const e={mode:l.value};await _n.downloadPythonScript(s.trial,e),r("exported",x.value),r("close")}catch(e){console.error("Export failed:",e),alert("Export failed: "+(e instanceof Error?e.message:"Unknown error"))}finally{m.value=!1}}return(e,n)=>{const s=p("a-button"),r=p("a-tag"),f=p("a-radio"),v=p("a-radio-group"),w=p("a-typography-text");return t(),y(L,{"model-value":!0,title:"Export Python Script",size:"full","onUpdate:modelValue":n[2]||(n[2]=n=>e.$emit("close"))},{footer:c(()=>[o(s,{onClick:n[0]||(n[0]=n=>e.$emit("close")),size:"large"},{default:c(()=>n[3]||(n[3]=[d(" Cancel ")])),_:1,__:[3]}),o(s,{type:"primary",onClick:P,loading:m.value,size:"large"},{default:c(()=>n[4]||(n[4]=[d(" Export Script ")])),_:1,__:[4]},8,["loading"])]),default:c(()=>[a("div",hn,[a("h3",null,i(e.trial.name),1),a("div",bn,[o(r,null,{default:c(()=>[d(i(u.value)+" API calls",1)]),_:1}),o(r,null,{default:c(()=>[d(i(g.value)+" configurations",1)]),_:1}),o(r,null,{default:c(()=>[d(i(_.value)+" variable combinations",1)]),_:1})])]),a("div",kn,[n[11]||(n[11]=a("h4",null,"Export Mode",-1)),o(v,{value:l.value,"onUpdate:value":n[1]||(n[1]=e=>l.value=e),class:"mode-options"},{default:c(()=>[o(f,{value:"simple",class:"mode-radio"},{default:c(()=>[a("div",xn,[n[5]||(n[5]=a("div",{class:"mode-title"},"Simple Script",-1)),n[6]||(n[6]=a("div",{class:"mode-description"}," Educational script with variables as lists. Easy to understand, modify, and extend. Perfect for learning how AI APIs work. ",-1)),o(r,{color:"blue",size:"small"},{default:c(()=>[d("~"+i(h.value)+"KB",1)]),_:1})])]),_:1}),o(f,{value:"literal",class:"mode-radio"},{default:c(()=>[a("div",Cn,[n[7]||(n[7]=a("div",{class:"mode-title"},"Literal Reproduction",-1)),n[8]||(n[8]=a("div",{class:"mode-description"}," Exact API calls pre-computed. Bit-for-bit reproduction of your experiment. Best for debugging and comparing results. ",-1)),o(r,{color:"blue",size:"small"},{default:c(()=>[d("~"+i(b.value)+"KB",1)]),_:1})])]),_:1}),o(f,{value:"native",class:"mode-radio"},{default:c(()=>[a("div",Pn,[n[9]||(n[9]=a("div",{class:"mode-title"},"Native Libraries",-1)),n[10]||(n[10]=a("div",{class:"mode-description"}," Uses official Python SDKs (openai, anthropic, ollama). Cleanest code, best for production use. Requires: pip install openai anthropic ollama ",-1)),o(r,{color:"green",size:"small"},{default:c(()=>[d("~"+i(k.value)+"KB",1)]),_:1})])]),_:1})]),_:1},8,["value"])]),n[13]||(n[13]=a("div",{class:"export-section"},[a("h4",null,"Output Format"),a("div",{class:"format-info"},[a("p",null,"Both scripts save results using pandas in your choice of format:"),a("ul",null,[a("li",null,[a("strong",null,"CSV"),d(" - Universal format, opens in Excel/Google Sheets")]),a("li",null,[a("strong",null,"Excel"),d(" - Native Excel format")]),a("li",null,[a("strong",null,"JSON"),d(" - For programmatic access")]),a("li",null,[a("strong",null,"Parquet"),d(" - Efficient compressed format")]),a("li",null,[a("strong",null,"HTML"),d(" - For web viewing")]),a("li",null,[a("strong",null,"Markdown"),d(" - For documentation")]),a("li",null,[a("strong",null,"Stata"),d(" - For statistical analysis")]),a("li",null,[a("strong",null,"Pickle"),d(" - Python native format")])])])],-1)),a("div",wn,[n[12]||(n[12]=a("h4",null,"Script Preview",-1)),a("div",Sn,[o(w,{code:"",class:"preview-filename"},{default:c(()=>[d(i(x.value),1)]),_:1}),a("div",In,[o(r,{size:"small"},{default:c(()=>[d(i(C.value)+" lines",1)]),_:1}),o(r,{size:"small"},{default:c(()=>[d(i(l.value)+" mode",1)]),_:1})])])])]),_:1,__:[13]})}}}),En={class:"api-call-modal"},On={class:"modal-header"},Tn={class:"modal-content"},Nn={class:"section"},Mn={class:"info-grid"},Rn={class:"info-item"},jn={class:"call-id"},$n={class:"info-item"},Fn={class:"info-item"},Dn={class:"info-item"},Un={key:0,class:"info-item"},Ln={key:1,class:"info-item"},qn={key:2,class:"info-item"},zn={class:"section"},Bn={class:"variables-detail"},Kn={class:"variable-value"},Yn={key:0,class:"attributes-section"},Gn={class:"attribute-items"},Jn={class:"section"},Vn={class:"prompt-display"},Hn={key:0,class:"section"},Wn={key:0,class:"response-info"},Xn={class:"info-grid"},Zn={class:"info-item"},Qn={class:"info-item"},et={key:1,class:"result-content"},nt={key:0,class:"error-result"},tt={class:"error-message"},at={key:1,class:"content-result"},st={class:"content-display"},rt={class:"section"},lt={class:"raw-data"},it={key:1,class:"section"},ot={class:"raw-data"},pt={class:"modal-footer"},ct=$(e({__name:"APICallDetailModal",props:{apiCall:{},trial:{}},emits:["close"],setup(e){const m=e,u=v(()=>{if(!m.apiCall.request)return"No request data";const e=JSON.parse(JSON.stringify(m.apiCall.request));return e.headers&&Object.keys(e.headers).forEach(n=>{const t=n.toLowerCase();(t.includes("authorization")||t.includes("api-key")||t.includes("x-api-key")||t.includes("bearer"))&&(e.headers[n]="[REDACTED]")}),JSON.stringify(e,null,2)});function f(){return m.trial&&m.trial.configurationSnapshots[m.apiCall.configurationIndex]&&m.trial.configurationSnapshots[m.apiCall.configurationIndex].name||`Configuration ${m.apiCall.configurationIndex+1}`}function g(e){const n="string"==typeof e?new Date(e):e;return isNaN(n.getTime())?"Invalid date":n.toLocaleString()}async function _(){const e={id:m.apiCall.id,status:m.apiCall.status,configuration:f(),variables:m.apiCall.variables,variableAttributes:m.apiCall.variableAttributes,prompt:m.apiCall.prompt,request:JSON.parse(u.value),response:m.apiCall.response,result:m.apiCall.result,created:m.apiCall.created,completed:m.apiCall.completed},n=JSON.stringify(e,null,2);try{if(navigator.clipboard&&navigator.clipboard.writeText)return await navigator.clipboard.writeText(n),void D.success("Details copied to clipboard!");const e=document.createElement("textarea");e.value=n,e.style.position="fixed",e.style.left="-999999px",e.style.top="-999999px",document.body.appendChild(e),e.focus(),e.select();const t=document.execCommand("copy");if(document.body.removeChild(e),!t)throw new Error("execCommand failed");D.success("Details copied to clipboard!")}catch(t){console.error("Failed to copy to clipboard:",t),prompt("Copy this text manually:",n)}}return(e,m)=>{const v=p("a-button");return t(),n("div",{class:"modal-overlay",onClick:m[2]||(m[2]=b(n=>e.$emit("close"),["self"]))},[a("div",En,[a("div",On,[m[3]||(m[3]=a("h2",null,"API Call Details",-1)),a("button",{class:"close-btn",onClick:m[0]||(m[0]=n=>e.$emit("close"))},"×")]),a("div",Tn,[a("div",Nn,[m[11]||(m[11]=a("h3",null,"Overview",-1)),a("div",Mn,[a("div",Rn,[m[4]||(m[4]=a("label",null,"Call ID:",-1)),a("span",jn,i(e.apiCall.id),1)]),a("div",$n,[m[5]||(m[5]=a("label",null,"Status:",-1)),a("span",{class:k(["status-badge",e.apiCall.status])},i(e.apiCall.status),3)]),a("div",Fn,[m[6]||(m[6]=a("label",null,"Configuration:",-1)),a("span",null,i(f()),1)]),a("div",Dn,[m[7]||(m[7]=a("label",null,"Created:",-1)),a("span",null,i(g(e.apiCall.created)),1)]),e.apiCall.completed?(t(),n("div",Un,[m[8]||(m[8]=a("label",null,"Completed:",-1)),a("span",null,i(g(e.apiCall.completed)),1)])):s("",!0),e.apiCall.completed?(t(),n("div",Ln,[m[9]||(m[9]=a("label",null,"Duration:",-1)),a("span",null,i((y=e.apiCall.completed.getTime()-e.apiCall.created.getTime(),y<1e3?`${y}ms`:`${(y/1e3).toFixed(1)}s`)),1)])):s("",!0),e.apiCall.response?.latencyMs?(t(),n("div",qn,[m[10]||(m[10]=a("label",null,"API Latency:",-1)),a("span",null,i(e.apiCall.response.latencyMs)+"ms",1)])):s("",!0)])]),a("div",zn,[m[13]||(m[13]=a("h3",null,"Variables",-1)),a("div",Bn,[(t(!0),n(r,null,l(Object.entries(e.apiCall.variables),([e,s])=>(t(),n("div",{key:e,class:"variable-item"},[a("label",null,i(e)+":",1),a("span",Kn,i(s),1)]))),128))]),e.apiCall.variableAttributes&&Object.keys(e.apiCall.variableAttributes).length>0?(t(),n("div",Yn,[m[12]||(m[12]=a("h4",null,"Variable Attributes",-1)),(t(!0),n(r,null,l(Object.entries(e.apiCall.variableAttributes),([e,s])=>(t(),n("div",{key:e,class:"attribute-group"},[a("h5",null,i(e),1),a("div",Gn,[(t(!0),n(r,null,l(Object.entries(s),([e,s])=>(t(),n("div",{key:e,class:"attribute-item"},[a("label",null,i(e)+":",1),a("span",null,i(s),1)]))),128))])]))),128))])):s("",!0)]),a("div",Jn,[m[14]||(m[14]=a("h3",null,"Resolved Prompt",-1)),a("div",Vn,i(e.apiCall.prompt),1)]),e.apiCall.response||e.apiCall.result?(t(),n("div",Hn,[m[19]||(m[19]=a("h3",null,"Response",-1)),e.apiCall.response?(t(),n("div",Wn,[a("div",Xn,[a("div",Zn,[m[15]||(m[15]=a("label",null,"HTTP Status:",-1)),a("span",null,i(e.apiCall.response.status),1)]),a("div",Qn,[m[16]||(m[16]=a("label",null,"Latency:",-1)),a("span",null,i(e.apiCall.response.latencyMs)+"ms",1)])])])):s("",!0),e.apiCall.result?(t(),n("div",et,[!1===e.apiCall.result.success?(t(),n("div",nt,[m[17]||(m[17]=a("h4",null,"Error",-1)),a("div",tt,i(e.apiCall.result.error),1)])):s("",!0),e.apiCall.result.content?(t(),n("div",at,[m[18]||(m[18]=a("h4",null,"Content",-1)),a("div",st,i(e.apiCall.result.content),1)])):s("",!0)])):s("",!0)])):s("",!0),a("div",rt,[m[20]||(m[20]=a("h3",null,"Raw Request",-1)),a("pre",lt,i(u.value),1)]),e.apiCall.response?(t(),n("div",it,[m[21]||(m[21]=a("h3",null,"Raw Response",-1)),a("pre",ot,i(JSON.stringify(e.apiCall.response,null,2)),1)])):s("",!0)]),a("div",pt,[o(v,{onClick:m[1]||(m[1]=n=>e.$emit("close")),size:"large",class:"footer-button"},{default:c(()=>m[22]||(m[22]=[d(" Close ")])),_:1,__:[22]}),o(v,{type:"primary",onClick:_,size:"large",class:"footer-button footer-button-primary"},{default:c(()=>m[23]||(m[23]=[d(" Copy Details ")])),_:1,__:[23]})])])]);var y}}}),[["__scopeId","data-v-595c884b"]]);export{ct as A,en as T,un as _,An as a,yn as p};
