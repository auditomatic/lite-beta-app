const e="ollama",a="Ollama",o="Local Ollama server - Run LLMs on your own hardware",n="http://localhost:11434",l={type:"none"},t={"Content-Type":"application/json"},r={defaultConcurrency:1,maxConcurrency:6,description:"Local GPU - default single model, max 6 due to browser HTTP/1.1 limit"},c={id:e,name:a,description:o,baseUrl:n,auth:l,headers:t,execution:r};export{l as auth,n as baseUrl,c as default,o as description,r as execution,t as headers,e as id,a as name};
