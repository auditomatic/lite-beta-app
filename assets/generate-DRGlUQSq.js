const e="ollama-generate",t="Ollama Generate",s="/api/generate",a="POST",n={promptKey:"prompt",wrapPrompt:!1,nestParams:"options",rootLevelParams:["stream"]},o={format:"ndjson",contentPaths:["response"],doneDetection:{field:"done",value:!0},finalResponseConstruction:{contentField:"response"}},i={promptTokensPath:"prompt_eval_count",completionTokensPath:"eval_count",totalTokensPath:"prompt_eval_count,eval_count"},r={text:{id:"text",label:"Text",description:"Standard text response",parameters:{},responseTransform:{contentPath:"response",fallbackPaths:["content","text"],errorPath:"error",validationRules:{conditions:[{type:"emptyWithReason",path:"done_reason",value:"load",errorMessage:"Response incomplete: model not loaded"}]}}}},p=[{pattern:".*",name:"All Ollama Models",params:{suffix:{type:"string",description:"Text after model response",basic:!1},system:{type:"string",description:"System message",basic:!1},template:{type:"string",description:"Prompt template",basic:!1},raw:{type:"boolean",description:"No formatting applied to prompt",default:!1,basic:!1},images:{type:"array",description:"Base64-encoded images for multimodal models",basic:!1},think:{type:"boolean",description:"Should thinking models think before responding",basic:!1},temperature:{type:"number",description:"Sampling temperature (0-2)",min:0,max:2,default:0,basic:!0},num_predict:{type:"integer",description:"Maximum tokens to generate",default:128,basic:!0,is_output_length:!0},num_keep:{type:"integer",description:"Number of tokens to keep from prompt",basic:!1},seed:{type:"integer",description:"Random seed for reproducible outputs",basic:!1},top_k:{type:"integer",description:"Top-k sampling",basic:!1},top_p:{type:"number",description:"Nucleus sampling threshold",min:0,max:1,basic:!1},min_p:{type:"number",description:"Minimum probability threshold",min:0,max:1,basic:!1},typical_p:{type:"number",description:"Typical probability threshold",min:0,max:1,basic:!1},repeat_last_n:{type:"integer",description:"Look back for repetition",basic:!1},repeat_penalty:{type:"number",description:"Repetition penalty",basic:!1},presence_penalty:{type:"number",description:"Presence penalty",basic:!1},frequency_penalty:{type:"number",description:"Frequency penalty",basic:!1},penalize_newline:{type:"boolean",description:"Penalize newline tokens",basic:!1},stop:{type:"array",description:"Stop sequences",basic:!1},numa:{type:"boolean",description:"Enable NUMA optimization",basic:!1},num_ctx:{type:"integer",description:"Context window size",basic:!1},num_batch:{type:"integer",description:"Batch size for token processing",basic:!1},num_gpu:{type:"integer",description:"Number of GPUs to use",basic:!1},main_gpu:{type:"integer",description:"Main GPU index",basic:!1},use_mmap:{type:"boolean",description:"Use memory mapping",basic:!1},num_thread:{type:"integer",description:"Number of threads to use",basic:!1},format:{type:"string",description:"Response format (json or JSON schema)",basic:!1},stream:{type:"boolean",description:"Streaming disabled (application cannot process NDJSON)",default:!1,required:!0,fixed:!1,basic:!1,hidden:!0}}}],l={id:e,name:t,endpoint:s,method:a,requestTransform:n,streamingTransform:o,usageExtraction:i,responseModes:r,modelRules:p};export{l as default,s as endpoint,e as id,a as method,p as modelRules,t as name,n as requestTransform,r as responseModes,o as streamingTransform,i as usageExtraction};
