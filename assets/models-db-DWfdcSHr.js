var e=Object.defineProperty,t=(t,o,a)=>((t,o,a)=>o in t?e(t,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):t[o]=a)(t,"symbol"!=typeof o?o+"":o,a);import{S as o,f as a,c as s}from"./vendor-DImCB_rW.js";import{l as r}from"./db-CL8uhZCz.js";import{p as i,l as n,d as l,c as u}from"./index-C3IQIdkL.js";class c{constructor(){t(this,"id","litellm"),t(this,"name","LiteLLM"),t(this,"LITELLM_URL","https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json")}async discover(){const e=await fetch(this.LITELLM_URL);if(!e.ok)throw new Error(`LiteLLM fetch failed: HTTP ${e.status}`);const t=await e.json();return this.processLiteLLMData(t)}processLiteLLMData(e){const t=[],o=new Date,a={openai:["openai-chat","openai-responses"],anthropic:["anthropic"]};for(const[s,r]of Object.entries(e)){const e=r.litellm_provider;if(e&&a[e])for(const l of a[e]){if(("openai-chat"===l||"openai-responses"===l)&&s.match(/(ft|image|dall|embedding|moderation|whisper|tts|transcribe|audio|realtime|search)/i))continue;const e=i.getProvider(l);if(!e)continue;const a=e.modelRules.find(e=>new RegExp(e.pattern).test(s));if(!a)continue;const u=this.extractCapabilities(s,r),c="anthropic"===l&&s.toLowerCase().includes("instant");c&&n.info("LiteLLM: Importing Anthropic instant model as disabled",{modelId:s}),t.push({id:`${l}:${s}`,provider:l,modelId:s,displayName:s,enabled:!c,source:"litellm",discoveredAt:o,modifiedAt:o,capabilities:u,parameterRules:a?{allows:"*",requires:a.requiredParams||[],forbids:a.forbidden||[],defaults:{}}:void 0,userModified:!1})}}return t}extractCapabilities(e,t){return{contextWindow:t.max_input_tokens||t.max_tokens,maxTokens:t.max_output_tokens||t.max_tokens,inputCostPerToken:t.input_cost_per_token?parseFloat(t.input_cost_per_token):0,outputCostPerToken:t.output_cost_per_token?parseFloat(t.output_cost_per_token):0,supportsFunctionCalling:t.supports_function_calling||!1,supportsJsonMode:this.inferJsonModeSupport(e,t),supportsStreaming:"bedrock"!==t.litellm_provider}}inferJsonModeSupport(e,t){return"openai"===t.litellm_provider&&(e.includes("gpt-4")||e.includes("gpt-3.5-turbo"))}}const d=["amazon/nova-lite-v1","amazon/nova-micro-v1","amazon/nova-pro-v1","anthropic/claude-3-haiku","anthropic/claude-3.5-haiku","anthropic/claude-3.5-sonnet","anthropic/claude-3.7-sonnet","anthropic/claude-3.7-sonnet:thinking","anthropic/claude-opus-4","anthropic/claude-sonnet-4","cohere/command-a","cohere/command-r","cohere/command-r-plus","deepseek/deepseek-chat","deepseek/deepseek-r1","google/gemini-2.0-flash-001","google/gemini-2.0-flash-lite-001","google/gemini-2.5-flash","google/gemini-2.5-flash-preview","google/gemini-2.5-flash-preview:thinking","google/gemini-2.5-pro","google/gemini-2.5-pro-preview","google/gemini-flash-1.5","google/gemini-flash-1.5-8b","google/gemini-pro-1.5","google/gemma-2-27b-it","google/gemma-2-9b-it","google/gemma-3-12b-it","google/gemma-3-27b-it","google/gemma-3-4b-it","meta-llama/llama-3-70b-instruct","meta-llama/llama-3-8b-instruct","meta-llama/llama-3.1-405b-instruct","meta-llama/llama-3.1-70b-instruct","meta-llama/llama-3.1-8b-instruct","meta-llama/llama-3.2-1b-instruct","meta-llama/llama-3.2-3b-instruct","meta-llama/llama-3.3-70b-instruct","meta-llama/llama-4-maverick","meta-llama/llama-4-scout","microsoft/phi-3-medium-128k-instruct","microsoft/phi-3-mini-128k-instruct","microsoft/phi-3.5-mini-128k-instruct","microsoft/phi-4","microsoft/phi-4-reasoning-plus","microsoft/wizardlm-2-8x22b","mistralai/ministral-3b","mistralai/ministral-8b","mistralai/mistral-7b-instruct-v0.3","mistralai/mistral-large","mistralai/mistral-large-2407","mistralai/mistral-large-2411","mistralai/mistral-small-24b-instruct-2501","mistralai/mistral-small-3.1-24b-instruct","mistralai/mistral-small-3.2-24b-instruct","mistralai/mistral-tiny","mistralai/mixtral-8x22b-instruct","mistralai/mixtral-8x7b-instruct","nvidia/llama-3.1-nemotron-70b-instruct","nvidia/llama-3.1-nemotron-ultra-253b-v1","nvidia/llama-3.3-nemotron-super-49b-v1","openai/gpt-3.5-turbo-0613","openai/gpt-4-0314","openai/gpt-4-1106-preview","openai/gpt-4.1","openai/gpt-4.1-mini","openai/gpt-4.1-nano","openai/gpt-4.5-preview","openai/gpt-4o-2024-05-13","openai/gpt-4o-2024-08-06","openai/gpt-4o-2024-11-20","openai/gpt-4o-mini-2024-07-18","openai/o1-mini-2024-09-12","openai/o1-preview-2024-09-12","openai/o1-pro","openai/o3","openai/o3-mini","openai/o3-mini-high","openai/o3-pro","openai/o4-mini","openai/o4-mini-high","perplexity/r1-1776","perplexity/sonar","perplexity/sonar-deep-research","perplexity/sonar-pro","perplexity/sonar-reasoning","perplexity/sonar-reasoning-pro","qwen/qwen-2-72b-instruct","qwen/qwen-2.5-72b-instruct","qwen/qwen-2.5-7b-instruct","qwen/qwen-2.5-vl-7b-instruct","qwen/qwen-max","qwen/qwen-plus","qwen/qwen-turbo","qwen/qwen3-14b","qwen/qwen3-235b-a22b","qwen/qwen3-32b","qwen/qwen3-8b","qwen/qwq-32b","x-ai/grok-2-1212","x-ai/grok-3","x-ai/grok-3-beta","x-ai/grok-3-mini","x-ai/grok-3-mini-beta","x-ai/grok-4"],m={defaultEnabledModels:d},p=Object.freeze(Object.defineProperty({__proto__:null,default:m,defaultEnabledModels:d},Symbol.toStringTag,{value:"Module"}));class f{constructor(){t(this,"id","openrouter"),t(this,"name","OpenRouter"),t(this,"defaultEnabledModels",new Set(m.defaultEnabledModels))}async discover(){const e=await fetch("https://openrouter.ai/api/v1/models");if(!e.ok)throw new Error(`OpenRouter fetch failed: HTTP ${e.status}`);const t=await e.json();return this.processOpenRouterData(t)}processOpenRouterData(e){const t=[],o=new Date,a=i.getProvider("openrouter");if(!a)throw new Error("OpenRouter provider not found in registry");for(const s of e.data||[]){const e=a.modelRules.find(e=>new RegExp(e.pattern).test(s.id)),r=parseFloat(s.pricing?.prompt||"0"),i=parseFloat(s.pricing?.completion||"0"),n=this.defaultEnabledModels.has(s.id);t.push({id:`openrouter:${s.id}`,provider:"openrouter",modelId:s.id,displayName:s.id,enabled:n,source:"openrouter",discoveredAt:o,modifiedAt:o,capabilities:{contextWindow:s.context_length||4096,maxTokens:s.top_provider?.max_completion_tokens,inputCostPerToken:r,outputCostPerToken:i,supportsJsonMode:s.supported_parameters?.includes("response_format")||!1,supportsFunctionCalling:s.supported_parameters?.includes("tools")||!1,supportsStreaming:!0},parameterRules:e?{allows:"*",requires:e.requiredParams||[],forbids:e.forbidden||[],defaults:{}}:void 0,userModified:!1})}return t}}class h{constructor(){t(this,"id","ollama"),t(this,"name","Ollama")}async discover(){const e=await l.settings.get("main"),t=e?.customBaseUrls["ollama-chat"]||"http://localhost:11434",o=await fetch(`${t}/api/tags`,{signal:AbortSignal.timeout(3e3)});if(!o.ok)throw new Error(`Ollama not available: HTTP ${o.status}`);const a=await o.json();return this.processOllamaData(a)}processOllamaData(e){const t=[],o=new Date;for(const a of e.models||[])for(const e of["ollama-chat","ollama-generate"]){const s=i.getProvider(e);if(!s)continue;const r=s.modelRules.find(e=>new RegExp(e.pattern).test(a.name));t.push({id:`${e}:${a.name}`,provider:e,modelId:a.name,displayName:a.name,enabled:!0,source:"ollama",discoveredAt:o,modifiedAt:o,parameterRules:r?{allows:"*",requires:r.requiredParams||[],forbids:r.forbidden||[],defaults:{}}:void 0,capabilities:{inputCostPerToken:0,outputCostPerToken:0,supportsStreaming:!0},userModified:!1})}return t}}const g=new class{constructor(){t(this,"sources"),t(this,"sourceStatus"),t(this,"discoveryPromise",null),this.sources=new Map([["litellm",new c],["openrouter",new f],["ollama",new h]]),this.sourceStatus=new Map;for(const[e]of this.sources)this.sourceStatus.set(e,{id:e,status:"idle"})}async discoverModels(e={}){if(this.discoveryPromise)return this.discoveryPromise;this.discoveryPromise=this.performDiscovery(e);try{return await this.discoveryPromise}finally{this.discoveryPromise=null}}async performDiscovery(e){const{sources:t=["litellm","openrouter","ollama"],forceRefresh:o=!1,onProgress:a}=e,s=[];let r=0;if(!o){const e=await l.models.count();if(e>0)return n.info("Skipping discovery: models already exist",{existingCount:e}),{sources:Array.from(this.sourceStatus.values()),totalModels:e,errors:[]}}for(const n of t)this.updateSourceStatus(n,"loading"),a?.({source:n,status:"loading",message:`Discovering ${n} models...`});const i=t.map(e=>this.discoverFromSource(e)),u=await Promise.allSettled(i),c=[],d=[];let m=!1;if(u.forEach((e,o)=>{const r=t[o];if("fulfilled"===e.status){const t=e.value;c.push(...t),"ollama"===r&&d.push(...t),this.updateSourceStatus(r,"success",void 0,t.length),a?.({source:r,status:"success",message:`Discovered ${t.length} models from ${r}`})}else{const t=e.reason?.message||"Unknown error";s.push(`${r}: ${t}`),"ollama"===r&&(m=!0),this.updateSourceStatus(r,"error",t),a?.({source:r,status:"error",error:t})}}),t.includes("ollama")){const e=w();if(m)await e.disableModelsBySource("ollama"),n.info("Disabled all Ollama models due to server unavailability");else if(d.length>0){const t=d.map(e=>e.id);await e.disableModelsBySource("ollama",t),n.info("Updated Ollama models availability",{available:d.length})}}return c.length>0&&(await l.models.bulkPut(c),r=c.length),{sources:Array.from(this.sourceStatus.values()),totalModels:r,errors:s}}async discoverFromSource(e){const t=this.sources.get(e);if(!t)throw new Error(`Unknown source: ${e}`);try{return await t.discover()}catch(o){throw"ollama"===e&&o instanceof Error?n.info("Ollama not available",{message:o.message}):n.error("Failed to discover from source",o,{sourceId:e}),o}}updateSourceStatus(e,t,o,a){const s=this.sourceStatus.get(e);s&&this.sourceStatus.set(e,{...s,status:t,error:o,modelCount:a,lastUpdated:new Date})}getSourceStatus(){return Array.from(this.sourceStatus.values())}async refreshSource(e){if("ollama"===e)try{const e=this.sources.get("ollama");if(!e)throw new Error("Ollama source not found");const t=await e.discover();t.length>0&&await l.models.bulkPut(t);const o=w(),a=t.map(e=>e.id);await o.disableModelsBySource("ollama",a),this.updateSourceStatus("ollama","success",void 0,t.length),n.info("Ollama refresh successful",{modelCount:t.length})}catch(t){const e=w();await e.disableModelsBySource("ollama");const o=t instanceof Error?t.message:"Unknown error";throw this.updateSourceStatus("ollama","error",o),n.info("Ollama refresh failed - disabled all Ollama models",{error:o}),t}else await this.discoverModels({sources:[e],forceRefresh:!0})}async enableOpenRouterFavorites(){const e=await u(()=>Promise.resolve().then(()=>p),void 0,import.meta.url),t=new Set(e.default.defaultEnabledModels),o=(await l.models.where("provider").equals("openrouter").toArray()).map(e=>({key:e.id,changes:{enabled:t.has(e.modelId),modifiedAt:new Date}}));await l.models.bulkUpdate(o)}async retryFailedSources(e=3){const t=Array.from(this.sourceStatus.values()).filter(e=>"error"===e.status).map(e=>e.id);if(0===t.length)return{sources:this.getSourceStatus(),totalModels:await l.models.count(),errors:[]};let o=0,a=1e3;for(;o<e&&t.length>0;){o++,n.info("Retry attempt for failed sources",{attempt:o,failedSources:t}),await new Promise(e=>setTimeout(e,a)),a*=2;const e=await this.discoverModels({sources:t,forceRefresh:!0});t.length=0,e.sources.forEach(e=>{"error"===e.status&&t.push(e.id)})}return{sources:this.getSourceStatus(),totalModels:await l.models.count(),errors:t.map(e=>`${e}: Max retries exceeded`)}}},w=o("models",()=>{const e=a([]),t=a(!0),o=a([]),i=a([]);let u=null;const c=s(()=>e.value),d=s(()=>{const t={};for(const o of e.value)t[o.provider]||(t[o.provider]=[]),t[o.provider].push(o);return t}),m=s(()=>e.value.filter(e=>e.enabled)),p=s(()=>e=>d.value[e]||[]),f=s(()=>(t,o)=>e.value.find(e=>e.id===`${t}:${o}`));async function h(e,t){await l.models.update(e,{...t,modifiedAt:new Date})}async function w(e=!1){t.value=!0,i.value=[];try{const t=await g.discoverModels({forceRefresh:e,onProgress:e=>{n.info("Discovery progress",{source:e.source,status:e.status,message:e.message||e.error})}});return o.value=t.sources,i.value=t.errors,t}catch(a){throw n.error("Model discovery failed",a),a}finally{t.value=!1}}async function v(){if(0===await l.models.where("enabled").equals(1).count()){n.info("No models enabled, enabling defaults");0===await l.models.count()?await w():await g.enableOpenRouterFavorites()}}return async function(){await l.open(),u=r(()=>l.models.toArray()).subscribe({next:o=>{e.value=o,t.value=!1},error:e=>{n.error("Models subscription error",e),t.value=!1}})}().then(()=>{v()}),{models:s(()=>e.value),isLoading:s(()=>t.value),discoveryStatus:s(()=>o.value),discoveryErrors:s(()=>i.value),allModels:c,modelsByProvider:d,enabledModels:m,getModelsForProvider:p,getModel:f,addModel:async function(e){const t=new Date,o={...e,discoveredAt:t,modifiedAt:t};await l.models.put(o)},addModels:async function(e){const t=new Date,o=e.map(e=>({...e,discoveredAt:t,modifiedAt:t}));await l.models.bulkPut(o)},updateModel:h,toggleModel:async function(e,t){await h(e,{enabled:t})},toggleProvider:async function(t,o){const a=e.value.filter(e=>e.provider===t).map(e=>({key:e.id,changes:{enabled:o,modifiedAt:new Date}}));await l.models.bulkUpdate(a)},deleteModel:async function(e){const t=await l.models.get(e);if(!t||"user"!==t.source)throw new Error("Can only delete user-created models");await l.models.delete(e)},disableModelsBySource:async function(t,o){const a=e.value.filter(e=>e.source===t&&e.enabled&&(!o||!o.includes(e.id)));if(a.length>0){const e=a.map(e=>({key:e.id,changes:{enabled:!1,modifiedAt:new Date}}));await l.models.bulkUpdate(e),n.info(`Disabled ${a.length} models from source: ${t}`)}},clearModels:async function(){await l.models.clear()},discoverModels:w,refreshSource:async function(e){try{await g.refreshSource(e),o.value=g.getSourceStatus()}catch(t){throw n.error("Failed to refresh source",t,{sourceId:e}),t}},enableDefaultModels:v,ensureDefaultsEnabled:v,cleanup:function(){u?.unsubscribe()}}});export{g as m,w as u};
