const e="ollama",a="Ollama",o="Local Ollama server - Run LLMs on your own hardware",n="http://localhost:11434",l={type:"none"},r={"Content-Type":"application/json"},t={defaultConcurrency:1,maxConcurrency:64,description:"Local GPU - default single model, configurable for powerful hardware"},c={id:e,name:a,description:o,baseUrl:n,auth:l,headers:r,execution:t};export{l as auth,n as baseUrl,c as default,o as description,t as execution,r as headers,e as id,a as name};
